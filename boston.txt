

    import pandas as pd
    import numpy as np
    import seaborn as sns
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    from keras.models import Sequential
    from keras.layers import Dense

    data = pd.read_csv(".\HousingData.csv")

    data.head()

          CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO   
    0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296     15.3  \
    1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242     17.8   
    2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242     17.8   
    3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222     18.7   
    4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622    3  222     18.7   

            B  LSTAT  MEDV  
    0  396.90   4.98  24.0  
    1  396.90   9.14  21.6  
    2  392.83   4.03  34.7  
    3  394.63   2.94  33.4  
    4  396.90    NaN  36.2  

    data.columns

    Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',
           'PTRATIO', 'B', 'LSTAT', 'MEDV'],
          dtype='object')

    data.head(n=10)

          CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD  TAX  PTRATIO   
    0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900    1  296     15.3  \
    1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671    2  242     17.8   
    2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671    2  242     17.8   
    3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622    3  222     18.7   
    4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622    3  222     18.7   
    5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622    3  222     18.7   
    6  0.08829  12.5   7.87   NaN  0.524  6.012   66.6  5.5605    5  311     15.2   
    7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505    5  311     15.2   
    8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821    5  311     15.2   
    9  0.17004  12.5   7.87   NaN  0.524  6.004   85.9  6.5921    5  311     15.2   

            B  LSTAT  MEDV  
    0  396.90   4.98  24.0  
    1  396.90   9.14  21.6  
    2  392.83   4.03  34.7  
    3  394.63   2.94  33.4  
    4  396.90    NaN  36.2  
    5  394.12   5.21  28.7  
    6  395.60  12.43  22.9  
    7  396.90  19.15  27.1  
    8  386.63  29.93  16.5  
    9  386.71  17.10  18.9  

    data.shape

    (506, 14)

    data.isnull().sum()

    CRIM       20
    ZN         20
    INDUS      20
    CHAS       20
    NOX         0
    RM          0
    AGE        20
    DIS         0
    RAD         0
    TAX         0
    PTRATIO     0
    B           0
    LSTAT      20
    MEDV        0
    dtype: int64

    data.describe()

                 CRIM          ZN       INDUS        CHAS         NOX          RM   
    count  486.000000  486.000000  486.000000  486.000000  506.000000  506.000000  \
    mean     3.611874   11.211934   11.083992    0.069959    0.554695    6.284634   
    std      8.720192   23.388876    6.835896    0.255340    0.115878    0.702617   
    min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   
    25%      0.081900    0.000000    5.190000    0.000000    0.449000    5.885500   
    50%      0.253715    0.000000    9.690000    0.000000    0.538000    6.208500   
    75%      3.560263   12.500000   18.100000    0.000000    0.624000    6.623500   
    max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   

                  AGE         DIS         RAD         TAX     PTRATIO           B   
    count  486.000000  506.000000  506.000000  506.000000  506.000000  506.000000  \
    mean    68.518519    3.795043    9.549407  408.237154   18.455534  356.674032   
    std     27.999513    2.105710    8.707259  168.537116    2.164946   91.294864   
    min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   
    25%     45.175000    2.100175    4.000000  279.000000   17.400000  375.377500   
    50%     76.800000    3.207450    5.000000  330.000000   19.050000  391.440000   
    75%     93.975000    5.188425   24.000000  666.000000   20.200000  396.225000   
    max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   

                LSTAT        MEDV  
    count  486.000000  506.000000  
    mean    12.715432   22.532806  
    std      7.155871    9.197104  
    min      1.730000    5.000000  
    25%      7.125000   17.025000  
    50%     11.430000   21.200000  
    75%     16.955000   25.000000  
    max     37.970000   50.000000  

    data.info()

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 506 entries, 0 to 505
    Data columns (total 14 columns):
     #   Column   Non-Null Count  Dtype  
    ---  ------   --------------  -----  
     0   CRIM     486 non-null    float64
     1   ZN       486 non-null    float64
     2   INDUS    486 non-null    float64
     3   CHAS     486 non-null    float64
     4   NOX      506 non-null    float64
     5   RM       506 non-null    float64
     6   AGE      486 non-null    float64
     7   DIS      506 non-null    float64
     8   RAD      506 non-null    int64  
     9   TAX      506 non-null    int64  
     10  PTRATIO  506 non-null    float64
     11  B        506 non-null    float64
     12  LSTAT    486 non-null    float64
     13  MEDV     506 non-null    float64
    dtypes: float64(12), int64(2)
    memory usage: 55.5 KB

    sns.histplot(data["MEDV"])

    <Axes: xlabel='MEDV', ylabel='Count'>

[]

    sns.boxplot(data["MEDV"])

    <Axes: >

[]

    # Preprocess the data
    x = data.drop('MEDV', axis=1)
    y = data['MEDV']
    scaler = StandardScaler()
    x = scaler.fit_transform(x)

    # Split the data into training and testing sets
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3)

    print('Training set shape:', X_train.shape, Y_train.shape)
    print('Testing set shape:', X_test.shape, Y_test.shape)

    Training set shape: (354, 13) (354,)
    Testing set shape: (152, 13) (152,)

    # Define the model architecture
    model = Sequential()
    model.add(Dense(128, activation='relu', input_shape=(13,)))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(1))

    C:\Python\lib\site-packages\keras\src\layers\core\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
      super().__init__(activity_regularizer=activity_regularizer, **kwargs)

    # Display the model summary
    print(model.summary())

    Model: "sequential"

    ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
    ┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
    ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
    │ dense (Dense)                        │ (None, 128)                 │           1,792 │
    ├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
    │ dense_1 (Dense)                      │ (None, 64)                  │           8,256 │
    ├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
    │ dense_2 (Dense)                      │ (None, 32)                  │           2,080 │
    ├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
    │ dense_3 (Dense)                      │ (None, 16)                  │             528 │
    ├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
    │ dense_4 (Dense)                      │ (None, 1)                   │              17 │
    └──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

     Total params: 12,673 (49.50 KB)

     Trainable params: 12,673 (49.50 KB)

     Non-trainable params: 0 (0.00 B)

    None

    # Compile the model
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])

    # Train the model
    history = model.fit(X_train, Y_train, epochs=20, batch_size=512, validation_data=(X_test, Y_test))

    Epoch 1/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 4s 4s/step - loss: 588.8392 - mean_absolute_error: 22.4772 - val_loss: 613.0370 - val_mean_absolute_error: 22.9269
    Epoch 2/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 176ms/step - loss: 583.0887 - mean_absolute_error: 22.3616 - val_loss: 612.8926 - val_mean_absolute_error: 22.9238
    Epoch 3/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 171ms/step - loss: 582.9479 - mean_absolute_error: 22.3585 - val_loss: 612.7488 - val_mean_absolute_error: 22.9206
    Epoch 4/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 149ms/step - loss: 582.8077 - mean_absolute_error: 22.3553 - val_loss: 612.6223 - val_mean_absolute_error: 22.9179
    Epoch 5/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 124ms/step - loss: 582.6843 - mean_absolute_error: 22.3526 - val_loss: 612.4961 - val_mean_absolute_error: 22.9151
    Epoch 6/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 132ms/step - loss: 582.5612 - mean_absolute_error: 22.3498 - val_loss: 612.3677 - val_mean_absolute_error: 22.9123
    Epoch 7/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 133ms/step - loss: 582.4360 - mean_absolute_error: 22.3470 - val_loss: 612.2365 - val_mean_absolute_error: 22.9095
    Epoch 8/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 148ms/step - loss: 582.3079 - mean_absolute_error: 22.3442 - val_loss: 612.1024 - val_mean_absolute_error: 22.9065
    Epoch 9/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 129ms/step - loss: 582.1772 - mean_absolute_error: 22.3412 - val_loss: 611.9658 - val_mean_absolute_error: 22.9035
    Epoch 10/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 135ms/step - loss: 582.0440 - mean_absolute_error: 22.3383 - val_loss: 611.8265 - val_mean_absolute_error: 22.9005
    Epoch 11/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 137ms/step - loss: 581.9082 - mean_absolute_error: 22.3352 - val_loss: 611.6845 - val_mean_absolute_error: 22.8974
    Epoch 12/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 145ms/step - loss: 581.7697 - mean_absolute_error: 22.3321 - val_loss: 611.5397 - val_mean_absolute_error: 22.8942
    Epoch 13/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 119ms/step - loss: 581.6285 - mean_absolute_error: 22.3289 - val_loss: 611.3921 - val_mean_absolute_error: 22.8910
    Epoch 14/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 141ms/step - loss: 581.4844 - mean_absolute_error: 22.3257 - val_loss: 611.2418 - val_mean_absolute_error: 22.8877
    Epoch 15/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 143ms/step - loss: 581.3379 - mean_absolute_error: 22.3224 - val_loss: 611.0884 - val_mean_absolute_error: 22.8844
    Epoch 16/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 140ms/step - loss: 581.1883 - mean_absolute_error: 22.3191 - val_loss: 610.9316 - val_mean_absolute_error: 22.8810
    Epoch 17/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 152ms/step - loss: 581.0354 - mean_absolute_error: 22.3157 - val_loss: 610.7735 - val_mean_absolute_error: 22.8775
    Epoch 18/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 118ms/step - loss: 580.8812 - mean_absolute_error: 22.3122 - val_loss: 610.6094 - val_mean_absolute_error: 22.8739
    Epoch 19/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 142ms/step - loss: 580.7211 - mean_absolute_error: 22.3086 - val_loss: 610.4434 - val_mean_absolute_error: 22.8703
    Epoch 20/20
    1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 159ms/step - loss: 580.5591 - mean_absolute_error: 22.3050 - val_loss: 610.2736 - val_mean_absolute_error: 22.8666

    # Evaluate the model on the testing set
    results = model.evaluate(X_test, Y_test)

    5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 626.0131 - mean_absolute_error: 23.0306  

    # Make predictions on the testing set
    predictions = model.predict(X_test)

    5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step

    predictions.shape

    (152, 1)

    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

    # Calculate Mean Squared Error (MSE)
    mse = mean_squared_error(Y_test, predictions)

    # Calculate Mean Absolute Error (MAE)
    mae = mean_absolute_error(Y_test, predictions)

    # Calculate Root Mean Squared Error (RMSE)
    rmse = np.sqrt(mse)

    # Calculate R-squared (R2)
    r2 = r2_score(Y_test, predictions)

    print("Mean Squared Error (MSE):", mse)
    print("Mean Absolute Error (MAE):", mae)
    print("Root Mean Squared Error (RMSE):", rmse)
    print("R-squared (R2):", r2)

    Mean Squared Error (MSE): 610.273548152246
    Mean Absolute Error (MAE): 22.866571061940565
    Root Mean Squared Error (RMSE): 24.70371527022294
    R-squared (R2): -5.983056126550371
